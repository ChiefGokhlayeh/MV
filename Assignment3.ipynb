{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "_IxxAKTfxxiU",
        "ouyODhArcfUL",
        "JFzeGgygOeX8",
        "ATTSTJH6P8ZX",
        "j5P7r_rHQ9hm",
        "Qrwavtq-Re_J",
        "ZzhYCKJYTiIb"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChiefGokhlayeh/MV/blob/main/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS7T20gWaEkg"
      },
      "source": [
        "# Machine Vision - Assignment 3: Object classification with HoG/SVM and Multilayer Perceptrons using public datasets\n",
        "\n",
        "---\n",
        "\n",
        "Prof. Dr. Markus Enzweiler, Esslingen University of Applied Sciences\n",
        "\n",
        "markus.enzweiler@hs-esslingen.de\n",
        "\n",
        "---\n",
        "\n",
        "This is the third assignment for the \"Machine Vision\" lecture. \n",
        "It covers:\n",
        "* training a pedestrian classifier using HoG features and linear SVM classifiers \n",
        "* training a neural network (MLP) to categorize an image into 1 out of 10 categories\n",
        "* working with public benchmark datasets ([Daimler Pedestrian Detection Benchmark](https://markus-enzweiler.de/datasets/) and [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html))\n",
        "\n",
        "**Make sure that \"GPU\" is selected in Runtime -> Change runtime type**\n",
        "\n",
        "To successfully complete this assignment, it is assumed that you already have some experience in Python and numpy. You can either use [Google Colab](https://colab.research.google.com/) for free with a private (dedicated) Google account (recommended) or a local Jupyter installation.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH3vu3HiqaZS"
      },
      "source": [
        "## Preparations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFz0s31SxNyP"
      },
      "source": [
        "### Import important libraries (you should probably start with these lines all the time ...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPcN62DgZ6Gg"
      },
      "source": [
        "# OpenCV\n",
        "import cv2   \n",
        "\n",
        "# NumPy                    \n",
        "import numpy as np   \n",
        "\n",
        "# glob\n",
        "import glob # glob\n",
        "\n",
        "# Matplotlib    \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "# make sure we show all plots directly below each cell\n",
        "%matplotlib inline \n",
        "\n",
        "# Some Colab specific packages\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  # image display\n",
        "  from google.colab.patches import cv2_imshow \n",
        "\n",
        "\n",
        "# scikit learn for SVM (support vector machines)\n",
        "from sklearn import svm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANRNsyDPTm9x"
      },
      "source": [
        "\n",
        "### Some helper functions that we will need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ5AIQr1TsHU"
      },
      "source": [
        "def my_imshow(image, windowTitle=\"Image\"):\n",
        "  '''\n",
        "  Displays an image and differentiates between Google Colab and a local Python installation. \n",
        "\n",
        "  Args: \n",
        "    image: The image to be displayed\n",
        "\n",
        "  Returns:\n",
        "    - \n",
        "  '''\n",
        "\n",
        "  if 'google.colab' in str(get_ipython()):\n",
        "    cv2_imshow(image)\n",
        "  else:\n",
        "    cv2.imshow(windowTitle, image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQJnwTJQPzvF"
      },
      "source": [
        "## Exercise 1 - Pedestrian Classification using HoG / linear SVM (10 points) \n",
        "\n",
        "In this exercise you will be developing a pedestrian classifier based on the famous [\"Histograms of Oriented Gradients (HoG) for Human Detection\" approach](https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf). The dataset you will be using is the public [\"Daimler Pedestrian Detection dataset\"](http://www.dariu.net/pami09.pdf) which is available in *pedestrianData.zip*.  \n",
        "\n",
        "For HoG features, you will be using OpenCV, e.g. the *HOGDescriptor* class in *cv2*. Hint: Use ```help(cv2.HOGDescriptor()``` to view the documentation. Linear Support Vector Machines to train in the HoG feature spaces are available via [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl29rs1LtUfK"
      },
      "source": [
        "### Unzip the dataset. It provides 9800 labeled images for training and 9800 labeled images for testing. (**PROVIDED**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yas3Z1jPzvG"
      },
      "source": [
        "################ Unzip the dataset in the Colab runtime #################\n",
        "import io\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/ChiefGokhlayeh/MV/main/data/pedestrian_detection/pedestrian_data.zip'\n",
        "\n",
        "response = requests.get(url, allow_redirects = True)\n",
        "stream = io.BytesIO(response.content)\n",
        "\n",
        "print(\"unzipping {}\".format(url))\n",
        "\n",
        "with zipfile.ZipFile(stream, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"pedestrian\")\n",
        "\n",
        "# training images\n",
        "trainPed    = glob.glob('pedestrian/data/train/ped_examples/' + \"*.pgm\")\n",
        "trainNonPed = glob.glob('pedestrian/data/train/non-ped_examples/' + \"*.pgm\")\n",
        "\n",
        "# test images\n",
        "testPed     = glob.glob('pedestrian/data/test/ped_examples/' + \"*.pgm\")\n",
        "testNonPed  = glob.glob('pedestrian/data/test/non-ped_examples/' + \"*.pgm\")\n",
        "\n",
        "print(\"trainPed    : {} image paths\".format(len(trainPed)))\n",
        "print(\"trainNonPed : {} image paths\".format(len(trainNonPed)))\n",
        "print(\"testPed     : {} image paths\".format(len(testPed)))\n",
        "print(\"testNonPed  : {} image paths\".format(len(testNonPed)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz1uQTbAtd3k"
      },
      "source": [
        "### Visualize a few images to get a feeling for what the data looks like (**PROVIDED**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vQhWw39tj-j"
      },
      "source": [
        "def displayRandomImages(dataSet):\n",
        "  indices = np.arange(len(dataSet))\n",
        "  np.random.shuffle(indices)\n",
        "  count=0\n",
        "  for i in indices[0:50]:\n",
        "      plt.subplot(5,10,count+1)\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "      plt.grid(False)\n",
        "      sampleImage = cv2.imread(dataSet[i])   \n",
        "      plt.imshow(sampleImage)\n",
        "      count = count+1\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# display some pedestrians\n",
        "print(\"Random pedestrians\")\n",
        "displayRandomImages(trainPed)\n",
        "\n",
        "# display some non-pedestrians\n",
        "print(\"Random non-pedestrians\")\n",
        "displayRandomImages(trainNonPed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feIA-pc1tnaa"
      },
      "source": [
        "### HoG feature transform (**add your code here**)\n",
        "\n",
        "Compute HoG features using OpenCV's ```cv2.HOGDescriptor.compute()``` function for all 4 image sets (trainPed, trainNonPed, testPed, testNonPed). Use the following parameters for HoG:\n",
        "\n",
        "* ```winSize = (18,36)```\n",
        "* ```blockSize = (6,6)```\n",
        "* ```blockStride = (3,3)```\n",
        "* ```cellSize = (3,3)```\n",
        "* ```nbins = 12```\n",
        "\n",
        "The result should be a 2640 dimensional HoG feature vector for every 18x36 pixel input image. Store the HoG feature representation in 4 matrices (one per image set) with the shape: numImages (rows) x featureDimension (cols). You will need to transpose the output of ```cv2.HOGDescriptor.compute()``` to make it a column vector. \n",
        "\n",
        "Your result should be four matrices of size 4800(5000) x 2640.  \n",
        "\n",
        "Hint: The [C++ documentation of HOGDescriptor()](https://docs.opencv.org/master/d5/d33/structcv_1_1HOGDescriptor.html#a5c8e8ce0578512fe80493ed3ed88ca83) might be helpful ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_6WrcMptu8w"
      },
      "source": [
        "# apply HoG feature transform to the 4 image sets\n",
        "\n",
        "# --------------------------------------------------#\n",
        "def applyHogTransform(image_set, hog_descriptor):\n",
        "\n",
        "  # feature dimension\n",
        "  hog_dim = hog_descriptor.getDescriptorSize()\n",
        "\n",
        "  # num_images x feature_dimension\n",
        "  hog_feature_set = np.zeros((len(image_set), hog_dim))\n",
        "\n",
        "  # loop through the image_set and compute HoG features for each image\n",
        "  for i, image_path in enumerate(image_set):\n",
        "    # read image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # compute HoG features\n",
        "    hog_feature_set[i, :] = hog_descriptor.compute(image).flatten()\n",
        " \n",
        "  print(\"Shape of feature matrix: {}\".format(hog_feature_set.shape))\n",
        "  return hog_feature_set\n",
        "# --------------------------------------------------#\n",
        "\n",
        "# create an instance of cv2.HOGDescriptor with the given parameters\n",
        "win_size = (18, 36)\n",
        "block_size = (6, 6)\n",
        "block_stride = (3, 3)\n",
        "cell_size = (3, 3)\n",
        "nbins = 12\n",
        "hog_descriptor = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
        "\n",
        "################################\n",
        "\n",
        "# apply transform to all four image sets\n",
        "trainPedHogFeatures    = applyHogTransform(trainPed,    hog_descriptor)\n",
        "trainNonPedHogFeatures = applyHogTransform(trainNonPed, hog_descriptor)\n",
        "testPedHogFeatures     = applyHogTransform(testPed,     hog_descriptor)\n",
        "testNonPedHogFeatures  = applyHogTransform(testNonPed,  hog_descriptor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFKvwPq6uM9s"
      },
      "source": [
        "### Linear SVM Training (**add your code here**)\n",
        "\n",
        "Train a linear support vector machine on the HoG representation of trainPed and trainNonPed using  [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC). Set the \"C\" parameter to ```C=0.1```. \n",
        "\n",
        "Training a linear SVM (see [svm.LinearSVC.fit()](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC.fit)) requires the **whole** training set (trainPedHogFeatures and trainNonPedHogFeatures) as a single matrix. Browse through [svm.LinearSVC.fit()](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC.fit) to find out on how to concatenate the data. \n",
        "\n",
        "Additionally, a vector of target class labels is required. Use the label ```1```for pedestrian samples and ```-1```for non-pedestrian samples. Hint: ```np.concatenate()``` and ```np.full()``` might come in handy ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLFh832ZuZWY"
      },
      "source": [
        "# Linear SVM Training\n",
        "import sklearn\n",
        "\n",
        "# create instance of LinearSVC\n",
        "svc = sklearn.svm.LinearSVC(C = 0.1)\n",
        "\n",
        "# training data\n",
        "features = np.vstack((trainPedHogFeatures, trainNonPedHogFeatures))\n",
        "\n",
        "# labels for the training data \n",
        "target = np.concatenate((np.full(len(trainPed), 1), np.full(len(trainNonPed), -1)))\n",
        "                                               \n",
        "# train SVM\n",
        "trained = svc.fit(features, target)\n",
        "\n",
        "###############################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKXMszNrvOV-"
      },
      "source": [
        "### Evaluate Your HoG / SVM Classifier (**add your code here**)\n",
        "\n",
        "Use [svm.LinearSVC.score()](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC.score) to compute the mean accuracy of your classifier on the training set and on the test set, both in the HoG feature space. \n",
        "\n",
        "Your classifier should reach approximately 99.4% accuracy on the training set and 75.4% accuracy on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKH_nafFvVbg"
      },
      "source": [
        "# get score on training set\n",
        "features = np.vstack((trainPedHogFeatures, trainNonPedHogFeatures))\n",
        "target = np.concatenate((np.full(len(trainPed), 1), np.full(len(trainNonPed), -1)))\n",
        "\n",
        "mean_accuracy = svc.score(features, target)\n",
        "print(\"Mean accuracy (training set) = {}%\".format(mean_accuracy))\n",
        "\n",
        "\n",
        "# get score on test set\n",
        "features = np.vstack((testPedHogFeatures, testNonPedHogFeatures))\n",
        "target = np.concatenate((np.full(len(testPed), 1), np.full(len(testNonPed), -1)))\n",
        "\n",
        "mean_accuracy = svc.score(features, target)\n",
        "print(\"Mean accuracy (test set) = {}%\".format(mean_accuracy))\n",
        "\n",
        "###############################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivFg5EcKVrTx"
      },
      "source": [
        "### Find the best and worst predictions of your HoG / SVM classifier (**PROVIDED**)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYeGgWJ8XKd7"
      },
      "source": [
        "# find the pedestrian test sample with the highest SVM prediction score\n",
        "predictions = svc.decision_function(testPedHogFeatures)\n",
        "best_index = np.argmax(predictions)\n",
        "sample_image = cv2.imread(testPed[best_index])  \n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(sample_image, cmap='gray') \n",
        "plt.title(\"PED: highest score at index {}: {}\".format(best_index, predictions[best_index]));\n",
        "\n",
        "# find the pedestrian test sample with the lowest SVM prediction score\n",
        "worst_index = np.argmin(predictions)\n",
        "sample_image = cv2.imread(testPed[worst_index])  \n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(sample_image, cmap='gray') \n",
        "plt.title(\"PED: lowest score at index {}: {}\".format(worst_index, predictions[worst_index]));\n",
        "\n",
        "# find the non-pedestrian test sample with the highest SVM prediction score\n",
        "predictions = svc.decision_function(testNonPedHogFeatures)\n",
        "best_index = np.argmin(predictions)\n",
        "sample_image = cv2.imread(testNonPed[best_index])  \n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(sample_image, cmap='gray') \n",
        "plt.title(\"NON-PED: highest score at index {}: {}\".format(best_index, predictions[best_index]));\n",
        "\n",
        "# find the non-pedestrian test sample with the lowest SVM prediction score\n",
        "worst_index = np.argmax(predictions)\n",
        "sample_image = cv2.imread(testNonPed[worst_index])  \n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(sample_image, cmap='gray') \n",
        "plt.title(\"NON-PED: lowest score at index {}: {}\".format(worst_index, predictions[worst_index]));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IxxAKTfxxiU"
      },
      "source": [
        "## Exercise 2 - CIFAR-10 Classification using Multilayer Perceptrons in TensorFlow / Keras (10 points)\n",
        "\n",
        "In this exercise you will train a multilayer perception neural network using TensorFlow and Keras on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. There will be no previous feature transform, i.e. the raw pixel values are the input to the neural network. Adam will be used as optimizer. \n",
        "\n",
        "[Keras](https://keras.io/) is a high-level API built on top of TensorFlow that provides an easier API to the training of neural networks in comparison to plain TensorFlow.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouyODhArcfUL"
      },
      "source": [
        "### Some more imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xomfQ80gchOI"
      },
      "source": [
        "# TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFzeGgygOeX8"
      },
      "source": [
        "### Getting familiar with the CIFAR-10 dataset (**PROVIDED**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXZ4tzXccZB9"
      },
      "source": [
        "# CIFAR-10 is available as standard dataset in Keras. Nice :)\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# load the data\n",
        "(trainSamples, _trainLabels), (testSamples, _testLabels) = cifar10.load_data()\n",
        "\n",
        "# scale the image data to float 0-1 (always recommended with neural networks)\n",
        "trainSamples = trainSamples.astype('float32') / 255.0\n",
        "testSamples  = testSamples.astype('float32') / 255.0\n",
        "\n",
        "# convert a class vector (integers) to binary class matrix.\n",
        "trainLabels  = to_categorical(_trainLabels)\n",
        "testLabels   = to_categorical(_testLabels)\n",
        "\n",
        "# text representation of class labels\n",
        "classNames = ['airplane', 'automobile', 'bird', \\\n",
        "               'cat', 'deer', 'dog', \\\n",
        "               'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Visualize 25 random images\n",
        "plt.figure(figsize=(10,10))\n",
        "indices = np.arange(len(trainSamples))\n",
        "np.random.shuffle(indices)\n",
        "count=0\n",
        "for i in indices[0:25]:\n",
        "    plt.subplot(5,5,count+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(trainSamples[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(\"label: {}\".format(classNames[np.argmax(trainLabels[i])]))\n",
        "    count = count+1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATTSTJH6P8ZX"
      },
      "source": [
        "### Neural Network Model Definition (**add your code here**)\n",
        "\n",
        "We want to design a standard \"feed-forward\" multilayer perceptron. In Keras-terms, this is referred to as a [sequential model](https://www.tensorflow.org/guide/keras/sequential_model). \n",
        "\n",
        "We will need the following layers (input to output):\n",
        "* 1 [Flatten](https://keras.io/api/layers/reshaping_layers/flatten/) layer that transforms our 32x32x3 pixel input to a 3072-dimensional vector\n",
        "* 4 [Dense](https://keras.io/api/layers/core_layers/dense/) hidden layers with 2048, 1024, 512, 64 neurons and ```relu```activation functions\n",
        "\n",
        "* 1 [Dense](https://keras.io/api/layers/core_layers/dense/) output layer with 10 neurons (1 per class) and ```softmax``` activation. \n",
        "\n",
        "[Adam](https://keras.io/api/optimizers/adam/) will be used as optimizer, see ```model.compile()```. As loss function we will use cross-entropy. \n",
        "\n",
        "Your ```model.summary()``` should look as follows (layer indices might differ):\n",
        "\n",
        "```_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "flatten_1 (Flatten)          (None, 3072)              0         \n",
        "_________________________________________________________________\n",
        "dense_5 (Dense)              (None, 2048)              6293504   \n",
        "_________________________________________________________________\n",
        "dense_6 (Dense)              (None, 1024)              2098176   \n",
        "_________________________________________________________________\n",
        "dense_7 (Dense)              (None, 512)               524800    \n",
        "_________________________________________________________________\n",
        "dense_8 (Dense)              (None, 64)                32832     \n",
        "_________________________________________________________________\n",
        "dense_9 (Dense)              (None, 10)                650       \n",
        "=================================================================\n",
        "Total params: 8,949,962\n",
        "Trainable params: 8,949,962\n",
        "Non-trainable params: 0\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrPJ4BVfQFIV"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "##### YOUR CODE GOES HERE ######\n",
        "# Define the layers of the MLP network model\n",
        "model = Sequential(\n",
        "    [\n",
        "     # input layer\n",
        "     layers.Flatten(input_shape=(32, 32, 3)),\n",
        "     # hidden layers\n",
        "     layers.Dense(2048, activation=\"relu\", name=\"dense_5\"),\n",
        "     layers.Dense(1024, activation=\"relu\", name=\"dense_6\"),\n",
        "     layers.Dense(512, activation=\"relu\", name=\"dense_7\"),\n",
        "     layers.Dense(64, activation=\"relu\", name=\"dense_8\"),\n",
        "     # output layer\n",
        "     layers.Dense(10, activation=\"softmax\", name=\"dense_9\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "################################\n",
        "\n",
        "# compile the model including optimizer and loss\n",
        "model.compile(optimizer=Adam(learning_rate=3e-4, beta_1=0.9, beta_2=0.999),\n",
        "             loss='categorical_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5P7r_rHQ9hm"
      },
      "source": [
        "### Neural Network Training (**add your code here**)\n",
        "\n",
        "Train your multilayer perceptron network using [model.fit()](https://keras.io/api/models/model_training_apis/). Pass ```trainSamples```and ```trainLabels```as training set and ```testSamples```and ```testLabels``` as ```validation_data```. \n",
        "\n",
        "Use the following hyper-parameters:\n",
        "* ```batch_size = 50```\n",
        "* ```epochs = 25```\n",
        "* ```verbose = 1```\n",
        "\n",
        "[model.fit()](https://keras.io/api/models/model_training_apis/) returns an history object which we will use later to visualize the behavior of the training and validation loss over time (epochs). \n",
        "\n",
        "The overall training should take about 5 seconds per epoch (**on a GPU**). Reported accuracies on the training (validation) data should be approx. 85% (53%) after 25 training epocs.   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsVNpyKvRAUC"
      },
      "source": [
        "history = model.fit(trainSamples,\n",
        "          trainLabels,\n",
        "          validation_data = (testSamples, testLabels),\n",
        "          batch_size = 50,\n",
        "          epochs = 25,\n",
        "          verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrwavtq-Re_J"
      },
      "source": [
        "### Visualize the behavior of the loss (**add your code here**)\n",
        "\n",
        "Using the ```history``` object returned by ```model.fit()```, plot the training loss and validation loss as a function of epochs.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsa77qfRRrDv"
      },
      "source": [
        "plt.figure(figsize = (12, 5))\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend((\"Traning Loss\", \"Validation Loss\"))\n",
        "plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzhYCKJYTiIb"
      },
      "source": [
        "### Run your network on some images to get predictions (**PROVIDED**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILGixiytTo2A"
      },
      "source": [
        "# select 50 images randomly from the test set and run them through the MLP\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "# 50 random images\n",
        "indices = np.arange(len(testSamples))\n",
        "np.random.shuffle(indices)\n",
        "count=0\n",
        "for i in indices[0:50]:\n",
        "    plt.subplot(5,10,count+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(testSamples[i], cmap=plt.cm.binary)\n",
        "\n",
        "    # predict MLP (need to reshape 32 x 32 x 3 pixels -> 1 x 3072 pixels)\n",
        "    prediction = model.predict(testSamples[i].reshape(1,32*32*3))\n",
        "   \n",
        "    # visualize true and predicted labels\n",
        "    groundTruthLabel = classNames[np.argmax(testLabels[i])]\n",
        "    predictedLabel   = classNames[np.argmax(prediction)]\n",
        "    plt.xlabel(\"T: {} / P: {}\".format(groundTruthLabel, predictedLabel))\n",
        "    count = count+1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}