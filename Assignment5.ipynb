{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5_solution.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "PE11CcpTosqZ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS7T20gWaEkg"
      },
      "source": [
        "# Machine Vision - Assignment 5: Generating Images with Variational Autoencoders\n",
        "\n",
        "---\n",
        "\n",
        "Prof. Dr. Markus Enzweiler, Esslingen University of Applied Sciences\n",
        "\n",
        "markus.enzweiler@hs-esslingen.de\n",
        "\n",
        "---\n",
        "\n",
        "This is the fifth assignment for the \"Machine Vision\" lecture. \n",
        "It covers:\n",
        "* training variational autoencoders and sampling them to generate new images\n",
        "* starting with existing TensorFlow / Keras code and adapting it to new problems\n",
        "* datasets used are [MNIST](http://yann.lecun.com/exdb/mnist/) and [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/)\n",
        "\n",
        "**Make sure that \"GPU\" is selected in Runtime -> Change runtime type**\n",
        "\n",
        "To successfully complete this assignment, it is assumed that you already have some experience in Python and numpy. You can either use [Google Colab](https://colab.research.google.com/) for free with a private (dedicated) Google account (recommended) or a local Jupyter installation.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQJnwTJQPzvF"
      },
      "source": [
        "## Exercise 1 - Using Variational Autoencoders (VAE) on MNIST (10 points)\n",
        "\n",
        "This exercise involves working with existing TensorFlow / Keras code and experimenting with the code. \n",
        "\n",
        "\n",
        "\n",
        "1.   Work through the [TensorFlow Convolutional Variational Autoencoder Tutorial](https://www.tensorflow.org/tutorials/generative/cvae) that trains a VAE on MNIST. It involves encoding 28x28 pixel MNIST images into **2(!) latent dimensions** and reconstructing the images from those 2 dimensions. Copy the tutorial to your Colab (or local) workspace and run the Jupyter notebook. Try to understand the main concepts in this tutorial, such as data, encoder/decoder CNN architecture, latent feature space, as we have seen in the lecture. **Do not get lost in details, such as the exact loss formulation and exact sampling procedure from the model (although it might be hard for TIB / SWB students ...).** \n",
        "2.   Make the following adaptations to the code:\n",
        "\n",
        "*   Visualize the original input test samples next to their reconstructed versions in the function ```generate_and_save_images()```\n",
        "*   Create a function to sample random samples from the VAE model. Sample 100 images and display them. *Hint: Have a look at ```model.sample()```*\n",
        "* Experiment with different values for the dimensions of the latent feature space. Besides 2 latent dimensions, also try 8, 32, and 128. How does the quality of the reconstructions and random samples change with the dimensionality of the latent feature space? *Hint: You need to disable ```plot_latent_images()``` for latent feature spaces with more than 2 dimensions.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY_ni4U0HNGn"
      },
      "source": [
        "## Exercise 2 - Generating virtual faces with Variational Autoencoders (10 points) \n",
        "\n",
        "---\n",
        "\n",
        "Adapt your code from exercise 1 to use the [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) dataset instead of MNIST. The goal is to train a VAE model that allows to reconstruct existing and sample novel face images. \n",
        "\n",
        "1. Integrate the code provided below to download, extract and preprocess the LFW dataset into your existing VAE MNIST code. We will be using images of size 64 x 64 pixels (instead of 28 x 28 pixels in the case of MNIST). Additionally, LFW involves three-channel RGB color images. We need to account for both differences in the encoder / decoder CNN architecture. Also, when displaying images, make sure to display all channels, e.g. ```plt.imshow(lfwDataset[i, :, :, :])```. \n",
        "\n",
        "2. Split the LFW dataset (13233 images) into 11233 training images and 2000 test images. *Hint: You can use [sklearn.model_selection.train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for that. Your output should then correspond to ```train_images``` and ```test_images``` in the VAE MNIST tutorial (exercise 1).*\n",
        "\n",
        "3. Adapt the encoder / decoder CNN architecture from the MNIST VAE as follows:\n",
        "\n",
        " **Encoder (64x64x3 images -> 2*32 latent dimensions (mean and variance per latent dimension))**\n",
        " *   [InputLayer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/InputLayer) with ```input_shape=(64,64,3)```(64x64 images with three channels, RGB)\n",
        " *   [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) with 32 filters, 3x3 kernels, strides of 2x2, and ReLU activation (tensor dimensions: 64x64x3 -> 31x31x32) \n",
        " *   [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) with 32 filters, 3x3 kernels, strides of 2x2, and ReLU activation (tensor dimensions: 31x31x32 -> 15x15x32) \n",
        " *   [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) with 64 filters, 3x3 kernels, strides of 2x2, and ReLU activation (tensor dimensions: 15x15x32 -> 7x7x64) \n",
        " *   [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) with 128 filters, 3x3 kernels, strides of 2x2, and ReLU activation (tensor dimensions: 7x7x64 -> 3x3x128) \n",
        " * [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten)  (tensor dimensions: 3x3x128 -> 1152)\n",
        " * [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) with ```latent_dim + latent_dim``` dimensions (predict mean and variance per latent dimension) (tensor dimensions: 1152 -> 64)\n",
        "\n",
        " The encoder should look as follows (output of ```self.encoder.summary()```):\n",
        "\n",
        " ```\n",
        "    Model: \"sequential_6\"\n",
        "    _________________________________________________________________\n",
        "    Layer (type)                 Output Shape              Param #   \n",
        "    =================================================================\n",
        "    conv2d_15 (Conv2D)           (None, 31, 31, 32)        896       \n",
        "    _________________________________________________________________\n",
        "    conv2d_16 (Conv2D)           (None, 15, 15, 32)        9248      \n",
        "    _________________________________________________________________\n",
        "    conv2d_17 (Conv2D)           (None, 7, 7, 64)          18496     \n",
        "    _________________________________________________________________\n",
        "    conv2d_18 (Conv2D)           (None, 3, 3, 128)         73856     \n",
        "    _________________________________________________________________\n",
        "    flatten_3 (Flatten)          (None, 1152)              0         \n",
        "    _________________________________________________________________\n",
        "    dense_6 (Dense)              (None, 64)                73792     \n",
        "    =================================================================\n",
        "    Total params: 176,288\n",
        "    Trainable params: 176,288\n",
        "    Non-trainable params: 0\n",
        " ```\n",
        "\n",
        " **Decoder (32 latent dimensions -> 64x64x3 images)**\n",
        "\n",
        " The overall decoder architecture uses the following principle: Fully connect 32 input neurons (number of latent feature dimensions) to a grid of 16x16x32 neurons. With transposed convolutions (with the *correct* strides, kernel sizes and padding) we can subsequently increase tensor dimensions from 16x16x32 up to 64x64x128. For the last layer, a convolutional layer with three different filter kernels of size 1x1x128 (!) are used to shrink the 64x64x128 tensor to 64x64x3, our output image size. 1x1 convolutions might not seem intutive, but they can be used to shrink down feature maps in the ```depth``` dimension, without affecting the spatial x,y dimensions of the output, e.g. see [this explanation](https://machinelearningmastery.com/introduction-to-1x1-convolutions-to-reduce-the-complexity-of-convolutional-neural-networks/) for more info on that (if you want ....). Remember that in CNNs all convolution filters always extend to the full depth of the input tensor volume, as we have seen in the lecture. \n",
        "\n",
        " *   [InputLayer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/InputLayer) with ```input_shape=(32)```(32 latent dimensions)\n",
        " * [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) with ```units=16*16*32, activation=tf.nn.relu``` (tensor dimensions: 32 -> 8192)\n",
        " * [Reshape](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape) with ```target_shape=(16, 16, 32)``` (tensor dimensions: 8192 -> 16x16x32)\n",
        " *  [Conv2DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose) with 64 filters, 3x3 kernels, strides of 2x2, \"same\" padding, and ReLU activation (tensor dimensions: 16x16x32 -> 32x32x64) \n",
        " *  [Conv2DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose) with 128 filters, 3x3 kernels, strides of 2x2, \"same\" padding, and ReLU activation (tensor dimensions: 32x32x64 -> 64x64x128) \n",
        " * [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) with 3 filters, 1x1 kernels, strides of 1x1, \"same\" padding and **no activation function** (tensor dimensions: 64x64x128 -> 64x64x3)\n",
        "\n",
        "  The decoder should look as follows (output of ```self.decoder.summary()```):\n",
        "\n",
        "    ```\n",
        "    Model: \"sequential_11\"\n",
        "    _________________________________________________________________\n",
        "    Layer (type)                 Output Shape              Param #   \n",
        "    =================================================================\n",
        "    dense_11 (Dense)             (None, 8192)              270336    \n",
        "    _________________________________________________________________\n",
        "    reshape_5 (Reshape)          (None, 16, 16, 32)        0         \n",
        "    _________________________________________________________________\n",
        "    conv2d_transpose_20 (Conv2DT (None, 32, 32, 64)        18496     \n",
        "    _________________________________________________________________\n",
        "    conv2d_transpose_21 (Conv2DT (None, 64, 64, 128)       73856     \n",
        "    _________________________________________________________________\n",
        "    conv2d_29 (Conv2D)           (None, 64, 64, 3)         387       \n",
        "    =================================================================\n",
        "    Total params: 363,075\n",
        "    Trainable params: 363,075\n",
        "    Non-trainable params: 0\n",
        "    ```\n",
        "\n",
        "\n",
        "\n",
        "4. Train your LFW VAE using 32 latent feature dimensions for 50 epochs on your training set. Similar to exercise 1, visualize the original input test samples next to their reconstructed versions in the function ```generate_and_save_images()``` and generate 100 virtual random face samples from your VAE model. Which features of persons (e.g. hair, head pose, skin color, clothing, ...) have been learned by the model and which features are not present?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE11CcpTosqZ"
      },
      "source": [
        "### Code for LFW dataset handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKohN4semP60"
      },
      "source": [
        "# Installs & Imports\n",
        "\n",
        "!pip install progress\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Python stuff\n",
        "import os\n",
        "import glob\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import time\n",
        "from IPython import display\n",
        "\n",
        "# OpenCV and other image handling\n",
        "import cv2   \n",
        "import imageio\n",
        "import PIL\n",
        "\n",
        "# NumPy                    \n",
        "import numpy as np   \n",
        "\n",
        "# TensorFlow\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Matplotlib    \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "# make sure we show all plots directly below each cell\n",
        "%matplotlib inline \n",
        "\n",
        "# scikit-learn\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxOp4pRViuGd"
      },
      "source": [
        "# Code for loading, extracting and pre-processing the LFW dataset. \n",
        "# Returns lfwDataset[i,:,:,:] with i being the image index.  \n",
        "\n",
        "\n",
        "def loadLFWDataset(imgWidth, imgHeight):\n",
        "  '''\n",
        "  Load, extract and pre-process the LFW dataset. Images will be converted to \n",
        "  RGB and re-scaled to 0-1 (float). \n",
        "\n",
        "  Args: \n",
        "    imgWidth: target image width\n",
        "    imgHeight: target image height\n",
        "\n",
        "  Returns:\n",
        "    lfwDataset[i,:,:,:] with i being the image index, followed by height, width, channels\n",
        "  '''\n",
        "\n",
        "  # load and untar the LFW dataset into the runtime\n",
        "  url = \"http://vis-www.cs.umass.edu/lfw/lfw.tgz\"\n",
        "  pathToTar = \"./lfw.tgz\"\n",
        "  print(\"Downloading LFW data from {} to {}\".format(url, pathToTar))\n",
        "  urllib.request.urlretrieve(url, pathToTar)\n",
        "\n",
        "  print(\"Extracting {}\".format(pathToTar))\n",
        "  with tarfile.open(pathToTar) as tar_ref:\n",
        "      tar_ref.extractall(\"\")\n",
        "\n",
        "  # get a list of paths to the individual images\n",
        "  images = glob.glob('lfw/*/' + \"*.jpg\")\n",
        "  print(\"Number of extracted images : {}\".format(len(images)))\n",
        "\n",
        "  # preprocess the images\n",
        "  lfwDataset = np.zeros([len(images), imgHeight, imgWidth, 3]).astype('float32')\n",
        "\n",
        "  i = 0\n",
        "  for imagePath in images:\n",
        "    # read image\n",
        "    image = cv2.imread(imagePath)  \n",
        "    # convert to RGB\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    # resize and rescale to 0-1 (float)\n",
        "    resized = cv2.resize(image, (imgHeight, imgWidth), interpolation = cv2.INTER_AREA)\n",
        "    lfwDataset[i,:,:,:] = (resized / 255.0).astype('float32')\n",
        "    i = i + 1\n",
        "    if not (i % 2500):\n",
        "      print(\"Preprocessing images: {} / {}\".format(i, len(images)))\n",
        "\n",
        "  print(\"Done \\nDataset shape is {}\".format(lfwDataset.shape))\n",
        "  return lfwDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjRYhi1Wkt-C"
      },
      "source": [
        "# load LFW dataset\n",
        "imgWidth  = 64\n",
        "imgHeight = 64\n",
        "lfwDataset = loadLFWDataset(imgWidth, imgHeight);\n",
        "numLfwImages = lfwDataset.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOykyNZdnMFp"
      },
      "source": [
        "# visualize some random samples from the dataset\n",
        "plt.figure(figsize=(10,10))\n",
        "indices = np.arange(numLfwImages)\n",
        "np.random.shuffle(indices)\n",
        "count=0\n",
        "for i in indices[0:25]:\n",
        "    plt.subplot(5,5,count+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(lfwDataset[i,:,:,:])\n",
        "    count = count+1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}