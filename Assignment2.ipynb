{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChiefGokhlayeh/MV/blob/main/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS7T20gWaEkg"
      },
      "source": [
        "# Machine Vision - Assignment 2: Convolution, Cartoonization and Panorama Stitching\n",
        "\n",
        "---\n",
        "\n",
        "Prof. Dr. Markus Enzweiler, Esslingen University of Applied Sciences\n",
        "\n",
        "markus.enzweiler@hs-esslingen.de\n",
        "\n",
        "---\n",
        "\n",
        "This is the second assignment for the \"Machine Vision\" lecture. \n",
        "It covers:\n",
        "* implementing your own image convolution function\n",
        "* applying OpenCV functions for Canny edge detection, bilateratal filtering, cartoonization\n",
        "* implementing an image panorama stitching function based on SIFT and RANSAC\n",
        "\n",
        "To successfully complete this assignment, it is assumed that you already have some experience in Python and numpy. You can either use [Google Colab](https://colab.research.google.com/) for free with a private (dedicated) Google account (recommended) or a local Jupyter installation.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH3vu3HiqaZS"
      },
      "source": [
        "## Preparations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFz0s31SxNyP"
      },
      "source": [
        "### Import important libraries (you should probably start with these lines all the time ...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPcN62DgZ6Gg"
      },
      "source": [
        "# OpenCV\n",
        "import cv2\n",
        "\n",
        "# NumPy\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "# make sure we show all plots directly below each cell\n",
        "%matplotlib inline\n",
        "\n",
        "# Some Colab specific packages\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  # image display\n",
        "  from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANRNsyDPTm9x"
      },
      "source": [
        "\n",
        "### Some helper functions that we will need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ5AIQr1TsHU"
      },
      "source": [
        "def my_imshow(image, windowTitle=\"Image\"):\n",
        "  '''\n",
        "  Displays an image and differentiates between Google Colab and a local Python installation. \n",
        "\n",
        "  Args: \n",
        "    image: The image to be displayed\n",
        "\n",
        "  Returns:\n",
        "    - \n",
        "  '''\n",
        "\n",
        "  if 'google.colab' in str(get_ipython()):\n",
        "    cv2_imshow(image)\n",
        "  else:\n",
        "    cv2.imshow(windowTitle, image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcxOS80YuDCD"
      },
      "source": [
        "### If you are not using Google Colab:\n",
        "Just store the image in the same folder as your Jupyter notebook file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQJnwTJQPzvF"
      },
      "source": [
        "## Exercise 1 - Image Convolution (10 points) \n",
        "\n",
        "*Note: This is approx. 20-25 lines of Python code to implement.*\n",
        "\n",
        "Implement a convolution function ```myConvolution(inImage, kernel)``` that takes in as input an arbitrary image and convolution kernel. Use only your own code, i.e. no OpenCV or other external libraries providing a dedicated convolution function. At the boundaries of the image, assume that the values outside the image are zero. A good strategy to do so is to pad the image with zeros prior to convolution  Padding involves adding additional rows and columns containing zeros at the image boundaries and removing the padding after convolution. \n",
        "\n",
        "Test your function with the provided image and kernel. Your output should look identical (maybe minor differences at the image boundaries due to different border handling) to the OpenCV reference ```cv2.filter2D()```that is provided. How does your implementation compare to the OpenCV implementation regarding computation time? You can use ```%%time``` for timing measurements. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bhV8Ca6r26f"
      },
      "source": [
        "---\n",
        "\n",
        "The following code contains a code template as well as an OpenCV implementation of image convolution as reference. Please add your own implementation to the function ```myConvolution(inImage, kernel)```. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl29rs1LtUfK"
      },
      "source": [
        "### Load and display the input image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yas3Z1jPzvG"
      },
      "source": [
        "# Load and display the input image \n",
        "\n",
        "# Path to the image (you will need to change that)\n",
        "imPath = \"/content/convolution/esslingen800px.jpg\"\n",
        "\n",
        "# read image and convert to gray\n",
        "inputImage = cv2.imread(imPath)\n",
        "inputImage = cv2.cvtColor(inputImage, cv2.COLOR_BGRA2GRAY)\n",
        "\n",
        "# Display the input image\n",
        "print(\"Input image\")\n",
        "my_imshow(inputImage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz1uQTbAtd3k"
      },
      "source": [
        "### Create a Gaussian filter kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vQhWw39tj-j"
      },
      "source": [
        "# Create a Gaussian filter kernel\n",
        "kernel = 1.0/16.0 * np.array([[1,2,1], [2,4,2], [1,2,1]])\n",
        "print(\"Gaussian Kernel\\n{}\".format(kernel))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feIA-pc1tnaa"
      },
      "source": [
        "### Apply the filter using OpenCV's ```filter2D()``` function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_6WrcMptu8w"
      },
      "source": [
        "%%time\n",
        "outputImage = cv2.filter2D(inputImage, -1, kernel, borderType=cv2.BORDER_CONSTANT)\n",
        "print(\"OpenCV filter2D() convolution result\")\n",
        "my_imshow(outputImage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFKvwPq6uM9s"
      },
      "source": [
        "### Implement your own convolution function ```myConvolution()``` (**add your code here**)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLFh832ZuZWY"
      },
      "source": [
        "# Template for your own convolution function\n",
        "\n",
        "def my_convolution(in_image, kernel):\n",
        "  '''\n",
        "  2D image convolution of an image with a filter kernel.\n",
        "\n",
        "  Args:   \n",
        "    in_image: input image\n",
        "    kernel: input 2D filter kernel\n",
        "\n",
        "  Returns:\n",
        "    out_image: output image, the input image convolved with the filter kernel\n",
        "  '''\n",
        "  # my_convolution is only defined for symetric kernels.\n",
        "  assert kernel.shape[0] == kernel.shape[1]\n",
        "  pad_width = int(kernel.shape[0] / 2) # required padding depends on kernel size\n",
        "\n",
        "  # Use numpy to add padding. Padding value is extrapolated from the image edge\n",
        "  # pixels (avoids bevel effect in output image).\n",
        "  padded_in_image = np.pad(in_image, pad_width, mode = 'edge')\n",
        "\n",
        "  # Allocate empty output buffer.\n",
        "  out_image = np.empty(in_image.shape)\n",
        "\n",
        "  # Enumerate through the 2d indicies of output buffer (y -> row, x -> column).\n",
        "  for y, x in np.ndindex(out_image.shape):\n",
        "    # Sum up the weighted image pixels. Due to padding in padded_in_image no\n",
        "    # offset needed.\n",
        "    out_image[y, x] = np.multiply(padded_in_image[y:y+kernel.shape[0], x:x+kernel.shape[1]], kernel).sum()\n",
        "\n",
        "  return out_image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKXMszNrvOV-"
      },
      "source": [
        "### Apply and time your own convolution function \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKH_nafFvVbg"
      },
      "source": [
        "%%time\n",
        "outputImage = my_convolution(inputImage, kernel)\n",
        "print(\"my_convolution() convolution result\")\n",
        "my_imshow(outputImage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6iVCqac-D_Q"
      },
      "source": [
        "## Exercise 2 - Canny Edge Detection and Cartoonization with OpenCV (10 points)\n",
        "\n",
        "*Note: This is approx. 10 lines of Python code to implement.*\n",
        "\n",
        "1.   Use your favorite search engine to find out how to do run the Canny Edge Detector in OpenCV, as we have seen in the lecture. Use it to detect edges in the provided image ```ironman.jpg``` and visualize the result. \n",
        "\n",
        "2.   Use your favorite search engine to find out how to do to cartoonization in OpenCV as a combination of edge detection and bilateral filtering, as we have seen in the lecture. Apply it to the image ```ironman.jpg``` and visualize the result. \n",
        "\n",
        "Your results should look similar to the following images (see ```result.jpg```). You can right-click on this image and \"open in new tab/window\" to display it in its original size:\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=16Zb5fsUfaGWi0YI39pezJ8vu-3UlWED4'></img>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oeCSJaRB0ZL"
      },
      "source": [
        "# Path to the image (you will need to change that)\n",
        "im_path = \"/content/canny_cartoonization/ironman.jpg\"\n",
        "\n",
        "# read image\n",
        "input_image = cv2.imread(im_path)\n",
        "\n",
        "# Display the input image\n",
        "print(\"Input image\")\n",
        "my_imshow(input_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkmOfr_359Rw"
      },
      "source": [
        "### Canny Edge Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hao9TsQzBA9A"
      },
      "source": [
        "canny_image = cv2.Canny(input_image, 130, 200)\n",
        "\n",
        "cv2_imshow(canny_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLZXWhyW6L7F"
      },
      "source": [
        "### Cartoonization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dxffv5ACRmn"
      },
      "source": [
        "bilateral_image = input_image\n",
        "bilateral_image = cv2.bilateralFilter(bilateral_image, 15, 40, 50)\n",
        "\n",
        "gray_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "canny_image = cv2.Canny(input_image, 130, 200)\n",
        "canny_image = cv2.cvtColor(canny_image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "cartoon_image = cv2.subtract(bilateral_image, canny_image)\n",
        "\n",
        "#cv2_imshow(cartoon_image)\n",
        "\n",
        "stack_image = np.hstack([input_image, canny_image, cartoon_image])\n",
        "\n",
        "cv2_imshow(stack_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdfZDFJENEYr"
      },
      "source": [
        "### Preparations and provided functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFzeGgygOeX8"
      },
      "source": [
        "#### Packages, Imports and Drive Mounting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUBZvB_CNU9V"
      },
      "source": [
        "# We will need to install a different opencv package and restart the Colab runtime, see below. \n",
        "# All the imports are reproduced here again so that you do not need to run anything from the \n",
        "# previous two exercises after restarting the Colab runtime. \n",
        "\n",
        "\n",
        "# Imports \n",
        "\n",
        "import sys\n",
        "import random\n",
        "\n",
        "# OpenCV\n",
        "import cv2   \n",
        "\n",
        "# NumPy                    \n",
        "import numpy as np     \n",
        "\n",
        "# Matplotlib    \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "# make sure we show all plots directly below each cell\n",
        "%matplotlib inline \n",
        "\n",
        "# Some Colab specific packages\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  # image display\n",
        "  from google.colab.patches import cv2_imshow \n",
        "\n",
        "\n",
        "\n",
        "# For SIFT, we need to install a more recent opencv version that what is \n",
        "# provided in the Colab runtime :(\n",
        "    \n",
        "OpenCVVersion = \"4.5.1.48\" # which version shall be installed?\n",
        "\n",
        "if 'google.colab' in sys.modules and (cv2.__version__ not in OpenCVVersion) :\n",
        "    import subprocess\n",
        "    print(\"Installing opencv-contrib-python version {}\".format(OpenCVVersion))\n",
        "    !pip uninstall opencv-python -y\n",
        "    !pip install opencv-contrib-python==$OpenCVVersion\n",
        "    print('Please manually restart Colab runtime ...')\n",
        "   \n",
        "print(\"Installed OpenCV version is {}\".format(cv2.__version__))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dipbUhbOk8N"
      },
      "source": [
        "#### Provided Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOrUmMD7OnGX"
      },
      "source": [
        "# Display an image\n",
        "def my_imshow(image, windowTitle=\"Image\"):\n",
        "  '''\n",
        "  Displays an image and differentiates between Google Colab and a local Python installation. \n",
        "\n",
        "  Args: \n",
        "    image: The image to be displayed\n",
        "\n",
        "  Returns:\n",
        "    - \n",
        "  '''\n",
        "\n",
        "  if 'google.colab' in str(get_ipython()):\n",
        "    cv2_imshow(image)\n",
        "  else:\n",
        "    cv2.imshow(windowTitle, image)\n",
        "\n",
        "\n",
        "\n",
        "# Provided function to compute an homography between two sets of points p1 and p2 based on least squares\n",
        "# Input format is the same as for cv2.findHomography()\n",
        "def myComputeHomography(p1, p2):\n",
        "  '''\n",
        "  Estimates an homography between two sets of points p1 and p2 based on least squares.  \n",
        "  Input format is the same as for cv2.findHomography(). At least 4 input points are needed in each input set. \n",
        "  '''\n",
        "  A = []\n",
        "  for i in range(0, len(p1)):\n",
        "      x, y = p1[i][0][0], p1[i][0][1]\n",
        "      u, v = p2[i][0][0], p2[i][0][1]\n",
        "      A.append([x, y, 1, 0, 0, 0, -u*x, -u*y, -u])\n",
        "      A.append([0, 0, 0, x, y, 1, -v*x, -v*y, -v])\n",
        "  A = np.asarray(A)\n",
        "  U, S, Vh = np.linalg.svd(A)\n",
        "  L = Vh[-1,:] / Vh[-1,-1]\n",
        "  return L.reshape(3, 3)\n",
        "\n",
        "\n",
        "\n",
        "# Provided function to stitch two images together using the provided homography\n",
        "def myStitchImages(imageLeft, imageRight, homography):\n",
        "  '''\n",
        "  Stitch two images together using the provided homography\n",
        "  '''\n",
        "  # Reserve some space for the final stitched image\n",
        "  stitchedImageHeight = imageLeft.shape[0] + imageRight.shape[0]\n",
        "  stitchedImageWidth  = imageLeft.shape[1] + imageRight.shape[1]\n",
        "\n",
        "  # Warp the right image \n",
        "  stitchedImage = cv2.warpPerspective(imageRight, homography, dsize = (stitchedImageWidth, stitchedImageHeight))\n",
        "\n",
        "  # Copy the left image into the stitched image\n",
        "  stitchedImage[0:imageLeft.shape[0], 0:imageLeft.shape[1], :] = imageLeft\n",
        "\n",
        "  return stitchedImage\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATTSTJH6P8ZX"
      },
      "source": [
        "### Compute and visualize SIFT keypoints and descriptors for both input images (**provided**) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrPJ4BVfQFIV"
      },
      "source": [
        "# Load the input images and convert to gray\n",
        "\n",
        "imPath = \"/content/panoramaStitching/\"\n",
        "\n",
        "inputImageLeftColor = cv2.imread(imPath + \"esslingenPanoLeft.jpg\")\n",
        "inputImageLeft      = cv2.cvtColor(inputImageLeftColor, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "inputImageRightColor = cv2.imread(imPath + \"esslingenPanoRight.jpg\")\n",
        "inputImageRight      = cv2.cvtColor(inputImageRightColor, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Display the input images\n",
        "print(\"Input image (left)\")\n",
        "my_imshow(inputImageLeftColor)\n",
        "\n",
        "print(\"Input image (right)\")\n",
        "my_imshow(inputImageRightColor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHdwb2uVQrfN"
      },
      "source": [
        "# Compute SIFT features for both the left and right image\n",
        "\n",
        "# Left image\n",
        "# Apply SIFT feature detector \n",
        "sift = cv2.SIFT_create()\n",
        "keyPointsLeft, descriptorsLeft = sift.detectAndCompute(inputImageLeft,None)\n",
        "siftImageLeft = cv2.drawKeypoints(inputImageLeft, keyPointsLeft, inputImageLeft, \\\n",
        "                                  flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "# Display image\n",
        "print(\"Found {} {}-dimensional SIFT Features (left)\".format(descriptorsLeft.shape[0], descriptorsLeft.shape[1]))\n",
        "my_imshow(siftImageLeft)\n",
        "\n",
        "# Right image\n",
        "# Apply SIFT feature detector \n",
        "sift = cv2.SIFT_create()\n",
        "keyPointsRight, descriptorsRight = sift.detectAndCompute(inputImageRight,None)\n",
        "siftImageRight = cv2.drawKeypoints(inputImageRight, keyPointsRight, inputImageRight, \\\n",
        "                                   flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "# Display image\n",
        "print(\"Found {} {}-dimensional SIFT Features (right)\".format(descriptorsRight.shape[0], descriptorsRight.shape[1]))\n",
        "my_imshow(siftImageRight)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5P7r_rHQ9hm"
      },
      "source": [
        "### Compute distances between every SIFT descriptor in one image and every descriptor in the other image. Select the top 2500 matches (**provided**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsVNpyKvRAUC"
      },
      "source": [
        "# Match SIFT descriptors between two images\n",
        "# Brute force matching: For every SIFT descriptor in the left image, the best matching \n",
        "# (= most similar SIFT descriptor) in the right image is found.\n",
        "# See: OpenCV BFMatcher class reference\n",
        "\n",
        "matcher = cv2.BFMatcher(normType = cv2.NORM_L2)\n",
        "matches = matcher.match(descriptorsLeft, descriptorsRight) \n",
        "print(\"# of matches: {}\".format(len(matches)))\n",
        "\n",
        "\n",
        "# Only take the best \"n\" matches, i.e. the ones with the smallest distance between SIFT descriptor vectors\n",
        "\n",
        "numMatchesUsed = 2500\n",
        "# Sort them in the order of their distance and only take \"n\"\n",
        "matches = sorted(matches, key = lambda x:x.distance)[:numMatchesUsed]\n",
        "\n",
        "# Extract the locations of matched keypoints in both images\n",
        "locationsLeft  = np.float32( [ keyPointsLeft  [match.queryIdx].pt for match in matches]).reshape(-1,1,2)\n",
        "locationsRight = np.float32( [ keyPointsRight [match.trainIdx].pt for match in matches]).reshape(-1,1,2)\n",
        "\n",
        "print(\"# of selected matches: {}\".format(len(locationsLeft)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrwavtq-Re_J"
      },
      "source": [
        "### OpenCV reference implementation of RANSAC-based homography estimation and stitching (**provided for reference**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsa77qfRRrDv"
      },
      "source": [
        "# RANSAC parameters (for both OpenCV and your implementation)\n",
        "\n",
        "rNumTrials = 250\n",
        "rMatchThresh = 7.0 # in pixels\n",
        "\n",
        "# OpenCV computed homography for reference\n",
        "openCVHomography, mask = cv2.findHomography(locationsRight, locationsLeft, cv2.RANSAC, \\\n",
        "                             ransacReprojThreshold = rMatchThresh, maxIters = rNumTrials)\n",
        "print(\"OpenCV computed homography (for reference) is: \\n{}\". format(openCVHomography))\n",
        "\n",
        "\n",
        "# Stitch\n",
        "stitchedImage = myStitchImages(inputImageLeftColor, inputImageRightColor, openCVHomography)\n",
        "\n",
        "print(\"Result using OpenCV reference implementation\")\n",
        "my_imshow(stitchedImage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzhYCKJYTiIb"
      },
      "source": [
        "### Your own RANSAC-based homography estimation and stitching (**to be implemented**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILGixiytTo2A"
      },
      "source": [
        "# RANSAC parameters\n",
        "r_num_trials = 250\n",
        "r_match_thresh = 7.0 # in pixels\n",
        "\n",
        "# variables\n",
        "inlier_fraction = 0\n",
        "r_best_inlier_fraction = 0 \n",
        "r_best_trial = 0\n",
        "\n",
        "# a dummy homography\n",
        "r_best_homography =  np.identity(3)\n",
        "r_best_homography[0,2] = 1500\n",
        "\n",
        "# Main RANSAC loop\n",
        "print(\"Starting RANSAC loop with {} iterations\".format(r_num_trials))\n",
        "\n",
        "for i_trial in range(r_num_trials):\n",
        "  # 1) Select 4 random matches out of the 2500. The actual point coordinates are in the \"keyPointsLeft\"\n",
        "  #    and \"keyPointsRight\" array. See how the 2500 matches were selected out of the full set. Now, we need\n",
        "  #    4 out of 2500 as 4 is the minimum number of points that is needed to compute an homography.\n",
        "  rand_indices = np.random.default_rng().integers(0, len(matches), 4)\n",
        "  rand_left = locationsLeft[rand_indices]\n",
        "  rand_right = locationsRight[rand_indices]\n",
        "  \n",
        "  # 2) Compute the homography using the provided myComputeHomography() function, see above.\n",
        "  homography = myComputeHomography(rand_right, rand_left)\n",
        "\n",
        "  # 3) Evaluate quality of the homography by computing the fraction of inliers through all 2500 matches. \n",
        "  #    Inliers are defined as the number of warped points from the right image (using the homography) that lie\n",
        "  #    within a user-defined distance \"r_match_thresh\" pixels of their corresponding point in image left image as \n",
        "  #    defined by the SIFT matching.\n",
        "\n",
        "  num_inliers = 0\n",
        "  # for all 2500 matches\n",
        "  for i_match in range(len(matches)):    \n",
        "    # Get the coordinates of left and right points for the current \"match\"\n",
        "    # (in homogeneous coordinates, i.e. a 3-dimensional vector with an added \"1.0\" as third element)\n",
        "  \n",
        "    left_hom = np.pad(locationsLeft[i_match], pad_width = ((0, 0), (0, 1)), constant_values = 1)\n",
        "    right_hom = np.pad(locationsRight[i_match], pad_width = ((0, 0), (0, 1)), constant_values = 1)\n",
        "    #print(f\"Homogenous coordinates left: {left_hom.reshape(3) if left_hom.shape[0] <= 1 else  left_hom} and right: {right_hom.reshape(3) if right_hom.shape[0] <= 1 else  right_hom}\")\n",
        "\n",
        "    # Warp the right point to the left image using using the homography. \n",
        "    # Convert back from homogenous coordinates by dividing the first and second element of the vector by the third\n",
        "    warped_to_left = np.matmul(homography, right_hom.T)\n",
        "    warped_to_left = (warped_to_left[:-1] / warped_to_left[-1]).T\n",
        "    #print(f\"Warping the right coordinate to the left: {warped_to_left.reshape(2) if warped_to_left.shape[0] <= 1 else  warped_to_left}\")\n",
        "   \n",
        "    # Compute euclidean distance between the original left point and the right point warped to the left image\n",
        "    dist = np.linalg.norm(locationsLeft[i_match] - warped_to_left)\n",
        "    #print(f\"Distance between left and warped-left: {dist}\")\n",
        "\n",
        "    # Inlier or outlier based on the euclidean distance and r_match_thresh\n",
        "    if dist <= r_match_thresh:\n",
        "      num_inliers += 1\n",
        "  # --------- loop over all matches ends here --------------\n",
        "\n",
        "  # 4) Use the fraction of inliers (num inliers / all matches) to evaluate the estimated homography. \n",
        "  #    If it is better than a previous one, store it.\n",
        "  inlier_fraction = num_inliers / len(matches)\n",
        "\n",
        "  if inlier_fraction > r_best_inlier_fraction:\n",
        "    r_best_inlier_fraction = inlier_fraction\n",
        "    r_best_homography = homography\n",
        "    r_best_trial = i_trial\n",
        "    \n",
        "  # 5) Some status output\n",
        "\n",
        "  print(\"---------------------------------------------------------------------------------------\") \n",
        "  print(\"RANSAC iteration [{}] : Found transform with {:0.2f}% inliers\".format(i_trial, inlier_fraction * 100))\n",
        "  print(\"Current best transform is in iteration {} with {:0.2f}% inliers\".format(r_best_trial, r_best_inlier_fraction * 100))\n",
        "\n",
        "# --------- end of a single RANSAC iteration --------------\n",
        "\n",
        "# 6) RANSAC loop (all iterations) completed\n",
        "\n",
        "print(\"---------------------------------------------------------------------------------------\") \n",
        "print(\"RANSAC loop completed\")\n",
        "print(\"Best transform is in iteration {} with {:0.2f}% inliers\".format(r_best_trial, r_best_inlier_fraction * 100))\n",
        "print(\"Estimated Homography: \\n{}\".format(r_best_homography))\n",
        "\n",
        "# !!!!! IMPORTANT !!!!!\n",
        "# Remove the following two lines after you have implemented RANSAC. It is a \"dummy homography\"\n",
        "#r_best_homography = np.identity(3)\n",
        "#r_best_homography[0,2] = 1500\n",
        "\n",
        "# Stitch using the best estimated homography\n",
        "stitchedImage = myStitchImages(inputImageLeftColor, inputImageRightColor, r_best_homography)\n",
        "\n",
        "print(\"Result using implemented RANSAC\")\n",
        "my_imshow(stitchedImage)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}